from pettingzoo import  ParallelEnv
from pettingzoo.utils import wrappers
import gymnasium as gym
from gym import spaces
import numpy as np
from numpy import inf
import random
import copy
from pettingzoo.utils import agent_selector, wrappers
from pettingzoo.utils.conversions import parallel_wrapper_fn
from pettingzoo.utils import BaseParallelWrapper
import time
import torch as T

def get_target_SOC(upcoming_millage, battery_capacity):
    nominal_efficiency = 2.2 #Estimated overall operating efficiency of the vehicle: 2.2 KWh / mile
    # Peak Power = Maximum power generated by the vehicle's motor: 250 KW
    
    energy_needed = upcoming_millage * nominal_efficiency
    buffer = battery_capacity * .10

    target_soc = energy_needed + buffer
    target_soc = min(target_soc, battery_capacity)
    return target_soc

def set_price_array(days_of_experiment, resolution, flag):
    intervals_per_hour = 60 // resolution
    intervals_per_day =  intervals_per_hour * 24
    intervals_total = days_of_experiment  * intervals_per_day
    price_array = np.zeros((intervals_total))
    price_flag = flag
    
        
    price_day = []
    
    if price_flag==1:
        Price_day = np.array([0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,
                     0.1, 0.1, 0.1, 0.05, 0.05, 0.05, 0.05])
    elif price_flag==2:
        Price_day=np.array([0.05, 0.05, 0.05, 0.05, 0.05, 0.06, 0.07, 0.08 ,0.09, 0.1, 0.1, 0.1, 0.08, 0.06, 0.05, 0.05, 0.05, 0.06, 0.06 ,0.06 ,0.06, 0.05, 0.05, 0.05])
    elif price_flag==3:
        Price_day = np.array([0.071, 0.060, 0.056, 0.056, 0.056, 0.060, 0.060, 0.060, 0.066, 0.066, 0.076, 0.080, 0.080, 0.1, 0.1, 0.076, 0.076,
                     0.1, 0.082, 0.080, 0.085, 0.079, 0.086, 0.070])
    elif price_flag==4:
       Price_day = np.array([0.1, 0.1, 0.05, 0.05, 0.05, 0.05, 0.05, 0.08, 0.08, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.06, 0.06, 0.06, 0.1, 0.1,
                    0.1, 0.1])
       
    repeated_hourly_prices = np.repeat(Price_day, intervals_per_hour)
    # Calculate scaling factor for each interval
    scaling_factor = 1 / intervals_per_hour
    # Interpolate prices for smaller intervals
    Energy_Prices = repeated_hourly_prices * scaling_factor
    return np.array(Energy_Prices)

def generate_departure_time(timestep, max_timestep):
    return random.randint(timestep, (max_timestep - 1))

def remove_agent(agents, depart_agents, EV):
    agents.remove(EV)
    depart_agents.append(EV)
    
def agent_reentry(depart_agents, EV, SOC, next_millage, target_SOC, agents, terminations):
    reentry_probability = 0.2
    for EV in depart_agents:
        if random.random( ) < reentry_probability:
            depart_agents.remove(EV)
            agents.append(EV)
            terminations[EV] = False
            SOC[EV] = random.uniform(45, 180)
            next_millage[EV] =  random.randint(60, 280)
            target_SOC[EV] = get_target_SOC(next_millage[EV], battery_capacity=450)
            
def charge_EV(rate, resolution):
        return rate * (resolution / 60) 
    
def calculate_charging_cost(energy_consumption, timestep, energy_prices):
        energy_cost = energy_prices[timestep]
        charging_cost = energy_cost * energy_consumption
        return charging_cost
    
def evaluate_charge_task(soc, target_soc):
    if  soc < target_soc:
        return ((target_soc - soc**2))
    else:
        return  0


class EV_CHARGE(gym.Env):
    def __init__(self, num_agents, alpha, beta, gamma):
        super(EV_CHARGE, self).__init__()
        self.action_space = spaces.Box(low=0.0, high=1.0, dtype=np.float32)
        self.observation_space = spaces.Box(low=0.0, high=1.0, shape=(1,5), dtype=np.float32)
        self.resolution = 15
        self.num_intervals = int((60/self.resolution)*24)
        self.battery_capacity = 450
        self.alpha = alpha
        self.beta = beta
        self.gamma = gamma
        self.delta = 0
        self.transformer_capacity = 1350 #1500kVa
        self.num_agents = num_agents
        
    def reset(self):
        self.done = False
        flag = 1
        self.rewards = 0
        self.cummulative_rewards = 0
        self.prev_rate = 0
        self.timestep = 0
        self.SOC = 0
        self.target_SOC = 0.8* self.battery_capacity
        self.safety_buffer = 0.9* self.battery_capacity
        self.departure_times = 70
        
        self.observation = [self.SOC, self.target_SOC, self.safety_buffer,
                            self.departure_times, self.prev_rate]
        return self.observation
    
    def step(self, actions):
        sufficient_battery_reward = 0
        overcharge_battery_penalty = 0
        under_charge_penalty = 0
        battery_penalty = 0
        charge_rate_penalty = 0
        self.rewards = 0
        self.rate = actions

        if self.timestep == self.departure_times + 1 or self.SOC == self.battery_capacity:
            self.done = True 
            return self.observation, self.rewards, self.done
        
        elif self.timestep == self.departure_times:
            #check undercharge
            if self.SOC < self.target_SOC:
                under_charge_penalty = (self.target_SOC - self.SOC)*.1 
            elif self.SOC  > self.target_SOC and self.SOC  < self.safety_buffer:
                sufficient_battery_reward = 100   
            else:
                overcharge_battery_penalty = (self.SOC - self.safety_buffer)*.1 
            self.rate = self.rate * 0

        if self.rate > 0:
            self.rate = self.rate*350
        else:
            self.rate = self.rate * 0
            
        charge_rate_penalty = self.rate * 0.01
        energy_consumed = charge_EV(self.rate, self.resolution)
        if isinstance(energy_consumed, np.ndarray):
            energy_consumed = energy_consumed.item()
        self.SOC = min(self.SOC+energy_consumed, self.battery_capacity)
        if self.SOC < self.target_SOC:
            battery_penalty = ((self.SOC-self.target_SOC)*2)*.01 # Negative reward linearly increasing towards 0
        
        elif self.SOC >= self.target_SOC and self.SOC<=self.safety_buffer:
            battery_penalty = 5  # Positive reward within target range
        else:
            battery_penalty = ((self.safety_buffer- self.SOC)*4)*.01  # Decreasing reward outside target range
        
        self.timestep += 1
        remaining_timesteps = self.num_intervals - self.timestep
        time_left = self.departure_times - self.timestep
        
        self.rewards = battery_penalty - charge_rate_penalty - overcharge_battery_penalty - under_charge_penalty + sufficient_battery_reward
        
        self.observation = np.array([self.SOC, self.target_SOC, self.safety_buffer, time_left, self.prev_rate], dtype=np.float32)

        
        self.prev_rate = self.rate.item() if isinstance(self.rate, np.ndarray) else self.rate
        self.cummulative_rewards += self.rewards
        
        return self.observation, self.rewards, self.done
    
