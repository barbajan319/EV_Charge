from pettingzoo import  ParallelEnv
from pettingzoo.utils import wrappers
import gymnasium 
from gymnasium.spaces import Box, Tuple, Discrete, Dict, MultiBinary, MultiDiscrete
import numpy as np
from numpy import inf
import random
import copy
from pettingzoo.utils import agent_selector, wrappers
from pettingzoo.utils.conversions import parallel_wrapper_fn
from pettingzoo.utils import BaseParallelWrapper
import time
import torch as T


def get_target_SOC(upcoming_millage, battery_capacity):
    nominal_efficiency = 2.2 #Estimated overall operating efficiency of the vehicle: 2.2 KWh / mile
    # Peak Power = Maximum power generated by the vehicle's motor: 250 KW
    
    energy_needed = upcoming_millage * nominal_efficiency
    buffer = battery_capacity * .10

    target_soc = energy_needed + buffer
    target_soc = min(target_soc, battery_capacity)
    return target_soc

def set_price_array(days_of_experiment, resolution, flag):
    intervals_per_hour = 60 // resolution
    intervals_per_day =  intervals_per_hour * 24
    intervals_total = days_of_experiment  * intervals_per_day
    price_array = np.zeros((intervals_total))
    price_flag = flag
    
        
    price_day = []
    
    if price_flag==1:
        Price_day = np.array([0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,
                     0.1, 0.1, 0.1, 0.05, 0.05, 0.05, 0.05])
    elif price_flag==2:
        Price_day=np.array([0.05, 0.05, 0.05, 0.05, 0.05, 0.06, 0.07, 0.08 ,0.09, 0.1, 0.1, 0.1, 0.08, 0.06, 0.05, 0.05, 0.05, 0.06, 0.06 ,0.06 ,0.06, 0.05, 0.05, 0.05])
    elif price_flag==3:
        Price_day = np.array([0.071, 0.060, 0.056, 0.056, 0.056, 0.060, 0.060, 0.060, 0.066, 0.066, 0.076, 0.080, 0.080, 0.1, 0.1, 0.076, 0.076,
                     0.1, 0.082, 0.080, 0.085, 0.079, 0.086, 0.070])
    elif price_flag==4:
       Price_day = np.array([0.1, 0.1, 0.05, 0.05, 0.05, 0.05, 0.05, 0.08, 0.08, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.06, 0.06, 0.06, 0.1, 0.1,
                    0.1, 0.1])
       
    repeated_hourly_prices = np.repeat(Price_day, intervals_per_hour)
    # Calculate scaling factor for each interval
    scaling_factor = 1 / intervals_per_hour
    # Interpolate prices for smaller intervals
    Energy_Prices = repeated_hourly_prices * scaling_factor
    return np.array(Energy_Prices)

def generate_departure_time(timestep, max_timestep):
    return random.randint(timestep, (max_timestep - 1))

def remove_agent(agents, depart_agents, EV):
    agents.remove(EV)
    depart_agents.append(EV)
    
def agent_reentry(depart_agents, EV, SOC, next_millage, target_SOC, agents, terminations):
    reentry_probability = 0.2
    for EV in depart_agents:
        if random.random( ) < reentry_probability:
            depart_agents.remove(EV)
            agents.append(EV)
            terminations[EV] = False
            SOC[EV] = random.uniform(45, 180)
            next_millage[EV] =  random.randint(60, 280)
            target_SOC[EV] = get_target_SOC(next_millage[EV], battery_capacity=450)
            
def charge_EV(rate, resolution):
        return rate * (resolution / 60) 
    
def calculate_charging_cost(energy_consumption, timestep, energy_prices):
        energy_cost = energy_prices[timestep]
        charging_cost = energy_cost * energy_consumption
        return charging_cost
    
def evaluate_charge_task(soc, target_soc):
    if  soc < target_soc:
        return ((target_soc - soc**2))
    else:
        return  0

class EV_CHARGE(object):
    
    def __init__(self, num_agents, alpha, beta, gamma):
        self.possible_agents = list(range(num_agents))
        self.agent_name_mapping = dict(
            zip(self.possible_agents, list(range(len(self.possible_agents))))
        )

        # self.num_stations = random.randint(1, len(self.possible_statagents))
        self.resolution = 15
        self.num_intervals = int((60/self.resolution)*24)
        self.battery_capacity = 450
        self.alpha = alpha
        self.beta = beta
        self.gamma = gamma
        self.delta = 0
        self.transformer_capacity = 1350 #1500kVa
        self.num_agents = num_agents
        self.observation_spaces = {
            agent: Dict(
                {
            "SOC": Box(low=0, high=350, shape=(1,), dtype=np.float32),
            "Target_SOC": Box(low=0, high=350, shape=(1,), dtype=np.float32),
            "Time_left_Depot": Discrete(self.num_intervals)#,
            #"Available_Stations": MultiBinary(self.num_stations)
        })
            for agent in self.possible_agents
            }
        
        self.action_spaces = {
            agent:
                Box(low=-1, high=1, shape=(1,), dtype=np.float32)
           for agent in self.possible_agents 
        }



    def reset(self, seed=None, options=None):
        #reset the environment to start again
        self.contributions = {}
        for i in range(self.num_agents):
            self.contributions[i] = {'undercharge': [0, 0.0], 'overcharge_bat': [0, []], 'charging_rate': [0, 0], 'gap': [0, 0]}  
            
        
        flag = 1 #random.choice([1, 2, 3, 4])
        self.agents = self.possible_agents[:]
        self.rewards = {agent: 0 for agent in self.agents}
        self.cummulative_rewards = {agent: 0 for agent in self.agents}
        
        self.terminations = {agent: False for agent in self.agents}
        self.truncations = False
        self.infos = {agent: {} for agent in self.agents}
        self.state = {agent: None for agent in self.agents}
        self.observations = {agent: None for agent in self.agents}
        self.indv_task = {agent: False for agent in self.agents}
        self.cummulative_charging_costs = 0
        self.completed_task_reward = 0
        self.task_completion = False
        self.group_reward_received = False 
        
        self.timestep  = 0
        self.transformer_load = 0 
        self.num_stations = len(self.agents) #random.randint(1, len(self.agents))
        self.charging_station_availability = {station: True for station in range(self.num_stations)} 
    
        #Initialize EVs battery randomly between 10% and 60%
        self.SOC = {agent: 45 for agent in self.agents} #{agent: random.uniform(45, 270) for agent in self.agents}
        #Determine each EVs next trip millage (Based on range of miles travelled by sandhaul)
        self.next_millage = {agent: 120 for agent in self.agents}  #{agent: random.randint(60, 205) for agent in self.agents} 
        #With the millage set the target SOC for next trip 
        self.target_SOC = {agent: get_target_SOC(self.next_millage[agent], self.battery_capacity) for agent in self.agents}
        #Randomize when EVs will depart for their first trip and time left at the depot
        self.departure_times = {agent:70 for agent in self.agents}#{agent: generate_departure_time(self.timestep + 8, self.num_intervals) for agent in self.agents}
        self.energy_prices = set_price_array(1, self.resolution, flag)
        self.assigned_station = {agent: None for agent in self.agents}
        self.charging_attempt = {agent: 0 for agent in self.agents}
        self.complete_charge = 0
        self.gap_time = {agent: 0 for agent in self.agents} 
        
        self.departed_agents = []
        charging_stations_available = [1 if value else 0 for value in self.charging_station_availability.values()]
        
        for agent in self.agents:
            self.observations[agent] = {
                "SOC": np.array([self.SOC[agent]/self.battery_capacity], dtype=np.float32),
                "Target_SOC": np.array([self.target_SOC[agent]/self.battery_capacity], dtype=np.float32),
                "Time_left_Depot": np.array([self.departure_times[agent]/self.num_intervals], dtype=np.float32)}
            
        return self.observations
    
    def step(self, actions):
        charging_stations_available = [1 if value else 0 for value in self.charging_station_availability.values()]
        sufficient_battery_reward = 0
        overcharge_battery_penalty = {agent: 0.0 for agent in self.possible_agents}
        overcharge_attempt_penalty = {agent: 0.0 for agent in self.possible_agents}
        undercharge_penalty = {agent: 0.0 for agent in self.possible_agents}
        gap_penalty = {agent: 0.0 for agent in self.possible_agents}
        task_rewards = {agent: 0.0 for agent in self.possible_agents}
        charge_rate_penalty = {agent: 0.0 for agent in self.possible_agents}
        self.agent_actions = {agent: 0 for agent in self.possible_agents}
        charge_request = []
        reward = 0
        
        for agent in self.departed_agents:
            self.terminations[agent] = True
            self.departed_agents.remove(agent)
            self.rewards[agent] = self.cummulative_rewards[agent]
            
        for agent in self.agents:
            action = actions[agent]
            if self.timestep == self.departure_times[agent]:
                if self.SOC[agent] < self.target_SOC[agent]:
                    undercharge_penalty[agent] = (self.target_SOC[agent] - self.SOC[agent]) * self.alpha        
                    self.contributions[agent]['undercharge'][0] += 1
                    self.contributions[agent]['undercharge'][1] += (self.target_SOC[agent] - self.SOC[agent])
                    self.rewards[agent] = -undercharge_penalty[agent]
                    self.cummulative_rewards[agent] -= undercharge_penalty[agent]
                    
                else:
                    self.complete_charge += 1
                    self.indv_task[agent] = True
                    task_rewards[agent] += sufficient_battery_reward
                    self.cummulative_rewards[agent] += self.rewards[agent] 
                    
                self.departed_agents.append(agent)
                action = 0
            
            if self.SOC[agent] == self.battery_capacity:
                action = 0
            
            if action > 0:
                charge_rate = action * 350
                
             
                if  self.SOC[agent] >= self.target_SOC[agent]:
                    overcharge_attempt_penalty[agent] = 0
                
                if self.assigned_station[agent] is None:
                    charge_request.append(agent)
                    
            else:
                charge_rate = 0
                
            
        
            self.agent_actions[agent] = charge_rate  
            self.contributions[agent]['charging_rate'][0] += 1
            self.contributions[agent]['charging_rate'][1] += charge_rate
            charge_rate_penalty[agent] = self.agent_actions[agent]*0.01
            
        
        if len(charge_request) <= sum(value == True for value in self.charging_station_availability.values()):
            for request in charge_request:
                for station, available in self.charging_station_availability.items():
                    if available:
                        self.charging_station_availability[station] = False
                        self.assigned_station[request] = station
                        break
                    
        else:
            for request in charge_request:
                self.agent_actions[request] = 0 
                
        for agent in self.agents:
            if self.agent_actions[agent] == 0:
                if self.assigned_station[agent] is not None:
                    station =  self.assigned_station[agent]
                    self.charging_station_availability[station] = True
                    self.assigned_station[agent] = None
                    
            else:
                charge_rate_agent = self.agent_actions[agent]
                energy_consumed = charge_EV(charge_rate_agent, self.resolution)
                self.SOC[agent] += energy_consumed
                #Check if their charging puts them over maximum amount
                if self.SOC[agent] >  self.battery_capacity:
                    self.SOC[agent] = self.battery_capacity
                    
            
            std_dev = 50  # Standard deviation of the Gaussian distribution
            peak_reward = 1 
            decay_factor = 0.03
    # Calculate the reward using a Gaussian distribution for the increasing part
            if self.SOC[agent] < self.target_SOC[agent]:
                reward = (peak_reward * np.exp(-0.5 * ((self.SOC[agent] - self.target_SOC[agent]) / std_dev) ** 2)) 
                reward = (reward - 1) * self.gamma
    # Calculate the reward for the decreasing part (mirror the Gaussian distribution)
    
            elif  self.SOC[agent] >= self.target_SOC[agent] and self.SOC[agent] <= (self.battery_capacity*0.85):

                reward = 1000
                
            elif self.SOC[agent] >= self.battery_capacity*0.86:
                print("battery",self.SOC[agent], self.target_SOC[agent], self.battery_capacity*0.86)
                reward = -(self.SOC[agent] - self.battery_capacity*0.86)*2
                
            # else:
            #     reward = peak_reward * np.exp(-0.5 * ((self.SOC[agent] - self.target_SOC[agent]) / std_dev) ** 2) *  np.exp(-decay_factor * (self.SOC[agent] - self.target_SOC[agent])) 
            #     reward = (reward - 1) * self.gamma
            gap_penalty[agent] = reward 

            if self.SOC[agent] > (self.target_SOC[agent]):
            #     overcharge_battery_penalty[agent] =  (self.SOC[agent] - self.target_SOC[agent]) * self.beta
                self.contributions[agent]['overcharge_bat'][0] += 1
                self.contributions[agent]['overcharge_bat'][1].append((self.SOC[agent] - self.target_SOC[agent]))
                

                    
            if  self.SOC[agent] <= self.target_SOC[agent] and self.timestep/self.departure_times[agent] != 1:
            #     # gap_penalty[agent] =((1 - (self.SOC[agent]/self.target_SOC[agent])) / (1 - (self.timestep/self.departure_times[agent]))) * self.gamma
            #     if self.SOC[agent] < self.target_SOC[agent]*.6:
            #         gap_penalty[agent] = -(1 - abs(self.SOC[agent] - self.target_SOC[agent])/self.target_SOC[agent])*self.gamma
                self.contributions[agent]['gap'][0] += 1
                self.contributions[agent]['gap'][1] +=  gap_penalty[agent] 
            #     else:
            #         gap_penalty[agent] =((1 - (self.SOC[agent]/self.target_SOC[agent])) / (1 - (self.timestep/self.departure_times[agent]))) * self.gamma
            #         self.contributions[agent]['gap'][0] += 1
            #         self.contributions[agent]['gap'][1] +=  gap_penalty[agent]
                
            # else:
            #     gap_penalty[agent] = 0
            
                    
            if agent in self.departed_agents:
                self.agents.remove(agent)
                
        self.timestep += 1
        remaining_timesteps = len(self.energy_prices) - self.timestep
        charging_stations_available = [1 if value else 0 for value in self.charging_station_availability.values()]
        
        for agent in self.possible_agents:
            time_left_depot = (self.departure_times[agent] - self.timestep)/self.num_intervals
            self.observations[agent] = {
                "SOC": np.array([self.SOC[agent]/self.battery_capacity], dtype=np.float32),
                "Target_SOC": np.array([self.target_SOC[agent]/self.battery_capacity], dtype=np.float32),
                "Time_left_Depot": np.array([time_left_depot], dtype=np.float32)#,
                #"Available_Stations": np.array(charging_stations_available)
            }
        
        for agent in self.agents:
            # self.rewards[agent] = task_rewards[agent] - overcharge_battery_penalty[agent] - undercharge_penalty[agent] + gap_penalty[agent]
            self.rewards[agent] = gap_penalty[agent] - charge_rate_penalty[agent]
            self.cummulative_rewards[agent] += self.rewards[agent]
            if self.SOC[agent] == self.battery_capacity:
                self.departed_agents.append(agent)
                self.agents.remove(agent)

        return self.observations, self.rewards, self.terminations, self.truncations, self.infos, self.task_completion 
        
