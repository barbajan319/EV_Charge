{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" RL Tutorial\n",
    "-Environment:\n",
    "    It is what we are trying to solve.\n",
    "    \n",
    "-Model:\n",
    "    What algorithm are we using? \n",
    "    \n",
    "-Agent:\n",
    "    Object that interacts with the environmnt intelligently \n",
    "    \n",
    "-Observation:\n",
    "    Important details about the environment that are fed to the model to make predictions\n",
    "    \n",
    "-Action:\n",
    "    The prediction of the model, which the agent then uses to take an action in the environment\n",
    "    \n",
    "-Step:\n",
    "    Take a step, pass action and action affects the environment\n",
    "    \n",
    "    \n",
    "Actions can either be DISCRETE or CONTINUOUS:\n",
    "-Discrete:\n",
    "    Clear classifications, you either go left or go right\n",
    "    \n",
    "-Continuous:\n",
    "    Like regression, a range of infinite possibilities \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample action: 3\n",
      "observation space shape (8,)\n",
      "sample obervation [ 11.171163   -74.26717     -0.55442965   1.7755046    2.1963074\n",
      "   2.329663     0.6091074    0.19216712]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.reset()\n",
    "\n",
    "print(\"sample action:\", env.action_space.sample())\n",
    "\n",
    "print(\"observation space shape\", env.observation_space.shape)\n",
    "\n",
    "print(\"sample obervation\", env.observation_space.sample())\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14087867641831053\n",
      "-1.2340391518668798\n",
      "-1.2551456327614392\n",
      "0.6543715161867294\n",
      "-0.3329664364051155\n",
      "-0.40509406084402144\n",
      "-1.7938379938092044\n",
      "0.4311606500873222\n",
      "-1.9106543736297386\n",
      "-1.888356738269947\n",
      "-0.1295533561066111\n",
      "-1.0824041385875205\n",
      "-2.2679143885212327\n",
      "-1.4110670625389616\n",
      "-2.5250975110029104\n",
      "-2.6290566131619912\n",
      "-1.155068996320597\n",
      "-3.064474125334782\n",
      "-1.152877184251895\n",
      "-1.0217029832975573\n",
      "-3.1548629128124217\n",
      "-2.9795760421879707\n",
      "-0.9315942623275635\n",
      "-0.8267710034952416\n",
      "-3.103144353226287\n",
      "-2.983343408675181\n",
      "-3.817123768923909\n",
      "-2.0913352060427712\n",
      "-3.6755890392939703\n",
      "-3.165095868209248\n",
      "-1.356533083138088\n",
      "-3.242850688141657\n",
      "-3.254584417014455\n",
      "-1.3788723919021482\n",
      "-4.1020320473992795\n",
      "-2.210249400820942\n",
      "-2.240408075682512\n",
      "-1.0919074537455618\n",
      "-3.0455974268373596\n",
      "-0.9767803730205447\n",
      "-2.202621981989341\n",
      "-1.994225893705675\n",
      "-2.0869457601298107\n",
      "-1.0805155608041719\n",
      "-0.5803609887101107\n",
      "-2.801257181538715\n",
      "-1.8119463530125586\n",
      "-2.650241942040593\n",
      "-1.9534200633553382\n",
      "-1.1739864746606645\n",
      "-2.8612197218473328\n",
      "-2.459395522493753\n",
      "-1.1343001705461393\n",
      "-0.5593046828356865\n",
      "-2.9887158279885737\n",
      "-2.3659827306188093\n",
      "-2.9050879288868985\n",
      "-0.9864220571649003\n",
      "-2.7969640109853047\n",
      "-0.722125856401276\n",
      "-3.310656165082537\n",
      "-2.7303186417487835\n",
      "-0.7821586484921272\n",
      "-0.4672233264392094\n",
      "-3.2513061119461097\n",
      "-0.21040945061949287\n",
      "-0.9609720425779642\n",
      "0.2584041512342605\n",
      "0.03925381224351668\n",
      "-2.2624683200534603\n",
      "-0.5959049614976379\n",
      "0.5547016650228056\n",
      "0.41862547840193653\n",
      "0.7054841190036154\n",
      "-0.06223647897667206\n",
      "-0.10755238365004516\n",
      "-2.3039629209049393\n",
      "-1.3187808200278528\n",
      "-1.722415924072236\n",
      "0.2056698869937759\n",
      "-2.670018127966318\n",
      "0.10267423465174488\n",
      "0.3398576464383336\n",
      "-1.2195540920088093\n",
      "-0.7711124818955568\n",
      "-3.2042280621652592\n",
      "-2.1116436928195115\n",
      "-0.4131476792286708\n",
      "-1.337045561056516\n",
      "-1.555050620794134\n",
      "-0.4988885740563933\n",
      "-1.6165822375707535\n",
      "-3.354124593222184\n",
      "-1.9303463776554963\n",
      "7.899298580124707\n",
      "85.6323698084108\n",
      "-5.060367922322086\n",
      "19.42464232063791\n",
      "7.449213776766187\n",
      "6.276819626832776\n",
      "5.37674193421708\n",
      "4.0298489109749775\n",
      "3.37770341969278\n",
      "5.549384122732449\n",
      "13.91776816531416\n",
      "-5.6780760894677655\n",
      "16.127527182221456\n",
      "4.531653722175521\n",
      "4.823918491887043\n",
      "5.439315004366915\n",
      "6.052535929965729\n",
      "-7.761954397134246\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\",  render_mode=\"human\")\n",
    "env.reset()\n",
    "\n",
    "for step in range(200):\n",
    "    env.render()\n",
    "    action, _states = model.predict(obs)\n",
    "    observation, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "    print(reward)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it discrete or continuos?\n",
    "\n",
    "There is discrete and multidiscrete\n",
    "Multidiscrete means many actions for example a robot that needs to control 8 servos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 103      |\n",
      "|    ep_rew_mean        | -289     |\n",
      "| time/                 |          |\n",
      "|    fps                | 47       |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.17    |\n",
      "|    explained_variance | -0.0133  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 2.93     |\n",
      "|    value_loss         | 25.7     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 110      |\n",
      "|    ep_rew_mean        | -341     |\n",
      "| time/                 |          |\n",
      "|    fps                | 47       |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 20       |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | 0.0357   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.599    |\n",
      "|    value_loss         | 2.72     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 105      |\n",
      "|    ep_rew_mean        | -424     |\n",
      "| time/                 |          |\n",
      "|    fps                | 47       |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 31       |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.00284  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -6.27    |\n",
      "|    value_loss         | 11       |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 111      |\n",
      "|    ep_rew_mean        | -451     |\n",
      "| time/                 |          |\n",
      "|    fps                | 47       |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 41       |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | -0.00106 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -12.8    |\n",
      "|    value_loss         | 212      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 118      |\n",
      "|    ep_rew_mean        | -426     |\n",
      "| time/                 |          |\n",
      "|    fps                | 47       |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 52       |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.981   |\n",
      "|    explained_variance | -0.00312 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 2.42     |\n",
      "|    value_loss         | 15.3     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 123       |\n",
      "|    ep_rew_mean        | -403      |\n",
      "| time/                 |           |\n",
      "|    fps                | 47        |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 62        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.18     |\n",
      "|    explained_variance | -2.57e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -11.6     |\n",
      "|    value_loss         | 245       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 128      |\n",
      "|    ep_rew_mean        | -386     |\n",
      "| time/                 |          |\n",
      "|    fps                | 47       |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 73       |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | -0.00361 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -1.95    |\n",
      "|    value_loss         | 2.09     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 132      |\n",
      "|    ep_rew_mean        | -375     |\n",
      "| time/                 |          |\n",
      "|    fps                | 47       |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 83       |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.24    |\n",
      "|    explained_variance | 0.156    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -5.03    |\n",
      "|    value_loss         | 122      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 132      |\n",
      "|    ep_rew_mean        | -375     |\n",
      "| time/                 |          |\n",
      "|    fps                | 48       |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 93       |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.00232  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.505    |\n",
      "|    value_loss         | 0.247    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 166      |\n",
      "|    ep_rew_mean        | -359     |\n",
      "| time/                 |          |\n",
      "|    fps                | 48       |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 103      |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.589   |\n",
      "|    explained_variance | -0.0101  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 8.59     |\n",
      "|    value_loss         | 22.8     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 168      |\n",
      "|    ep_rew_mean        | -342     |\n",
      "| time/                 |          |\n",
      "|    fps                | 48       |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 114      |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.517   |\n",
      "|    explained_variance | -0.00286 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -3.74    |\n",
      "|    value_loss         | 41.1     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 167      |\n",
      "|    ep_rew_mean        | -323     |\n",
      "| time/                 |          |\n",
      "|    fps                | 48       |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 124      |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.663   |\n",
      "|    explained_variance | -0.00211 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -0.83    |\n",
      "|    value_loss         | 11.1     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 169      |\n",
      "|    ep_rew_mean        | -308     |\n",
      "| time/                 |          |\n",
      "|    fps                | 48       |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 134      |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.842   |\n",
      "|    explained_variance | 0.00395  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | -5.54    |\n",
      "|    value_loss         | 70.4     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 172      |\n",
      "|    ep_rew_mean        | -301     |\n",
      "| time/                 |          |\n",
      "|    fps                | 48       |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 145      |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.48    |\n",
      "|    explained_variance | 0.000793 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | -3.89    |\n",
      "|    value_loss         | 43.7     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 178      |\n",
      "|    ep_rew_mean        | -281     |\n",
      "| time/                 |          |\n",
      "|    fps                | 48       |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 156      |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.00757  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -0.0452  |\n",
      "|    value_loss         | 1.32     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 178      |\n",
      "|    ep_rew_mean        | -277     |\n",
      "| time/                 |          |\n",
      "|    fps                | 48       |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 166      |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.55    |\n",
      "|    explained_variance | 0.21     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | -1.36    |\n",
      "|    value_loss         | 8.5      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 186      |\n",
      "|    ep_rew_mean        | -268     |\n",
      "| time/                 |          |\n",
      "|    fps                | 48       |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 176      |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.311   |\n",
      "|    explained_variance | 0.00522  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | -0.148   |\n",
      "|    value_loss         | 9.3      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 187       |\n",
      "|    ep_rew_mean        | -258      |\n",
      "| time/                 |           |\n",
      "|    fps                | 48        |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 187       |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.821    |\n",
      "|    explained_variance | -0.000737 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -2.51     |\n",
      "|    value_loss         | 27        |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 192      |\n",
      "|    ep_rew_mean        | -253     |\n",
      "| time/                 |          |\n",
      "|    fps                | 47       |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 198      |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.524   |\n",
      "|    explained_variance | 0.0048   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -13.6    |\n",
      "|    value_loss         | 95.1     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 196      |\n",
      "|    ep_rew_mean        | -250     |\n",
      "| time/                 |          |\n",
      "|    fps                | 47       |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 208      |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.469   |\n",
      "|    explained_variance | -0.00938 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 0.205    |\n",
      "|    value_loss         | 5.45     |\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import A2C \n",
    "\n",
    "env = gym.make(\"LunarLander-v2\",  render_mode=\"human\")\n",
    "env.reset()\n",
    "\n",
    "model = A2C(\"MlpPolicy\", env, verbose = 1)\n",
    "model.learn(total_timesteps = 10000)\n",
    "\n",
    "\n",
    "\n",
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    vec_env.render(\"human\")\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
