from pettingzoo import  ParallelEnv
from pettingzoo.utils import wrappers
import gymnasium 
from gymnasium.spaces import Box, Tuple, Discrete, Dict, MultiBinary
import numpy as np
from numpy import inf
from torch.utils.tensorboard import SummaryWriter
import random
import copy
from pettingzoo.utils import agent_selector, wrappers
from pettingzoo.utils.conversions import parallel_wrapper_fn
from pettingzoo.utils import BaseParallelWrapper
import time
import torch as T


def get_target_SOC(upcoming_millage, battery_capacity):
    nominal_efficiency = 2.2 #Estimated overall operating efficiency of the vehicle: 2.2 KWh / mile
    # Peak Power = Maximum power generated by the vehicle's motor: 250 KW
    
    energy_needed = upcoming_millage * nominal_efficiency
    buffer = battery_capacity * .10

    target_soc = energy_needed + buffer
    target_soc = min(target_soc, battery_capacity)
    return target_soc

def set_price_array(days_of_experiment, resolution, flag):
    intervals_per_hour = 60 // resolution
    intervals_per_day =  intervals_per_hour * 24
    intervals_total = days_of_experiment  * intervals_per_day
    price_array = np.zeros((intervals_total))
    price_flag = flag
    
        
    price_day = []
    
    if price_flag==1:
        Price_day = np.array([0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,
                     0.1, 0.1, 0.1, 0.05, 0.05, 0.05, 0.05])
    elif price_flag==2:
        Price_day=np.array([0.05, 0.05, 0.05, 0.05, 0.05, 0.06, 0.07, 0.08 ,0.09, 0.1, 0.1, 0.1, 0.08, 0.06, 0.05, 0.05, 0.05, 0.06, 0.06 ,0.06 ,0.06, 0.05, 0.05, 0.05])
    elif price_flag==3:
        Price_day = np.array([0.071, 0.060, 0.056, 0.056, 0.056, 0.060, 0.060, 0.060, 0.066, 0.066, 0.076, 0.080, 0.080, 0.1, 0.1, 0.076, 0.076,
                     0.1, 0.082, 0.080, 0.085, 0.079, 0.086, 0.070])
    elif price_flag==4:
       Price_day = np.array([0.1, 0.1, 0.05, 0.05, 0.05, 0.05, 0.05, 0.08, 0.08, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.06, 0.06, 0.06, 0.1, 0.1,
                    0.1, 0.1])
       
    repeated_hourly_prices = np.repeat(Price_day, intervals_per_hour)
    # Calculate scaling factor for each interval
    scaling_factor = 1 / intervals_per_hour
    # Interpolate prices for smaller intervals
    Energy_Prices = repeated_hourly_prices * scaling_factor
    return np.array(Energy_Prices)

def generate_departure_time(timestep, max_timestep):
    return random.randint(timestep, max_timestep)

def remove_agent(agents, depart_agents, EV):
    agents.remove(EV)
    depart_agents.append(EV)
    
def agent_reentry(depart_agents, EV, SOC, next_millage, target_SOC, agents, terminations):
    reentry_probability = 0.2
    for EV in depart_agents:
        if random.random( ) < reentry_probability:
            depart_agents.remove(EV)
            agents.append(EV)
            terminations[EV] = False
            SOC[EV] = random.uniform(45, 180)
            next_millage[EV] =  random.randint(60, 280)
            target_SOC[EV] = get_target_SOC(next_millage[EV], battery_capacity=450)
            

def charge_EV(rate, resolution):
        return rate * (resolution / 60) 
    
def calculate_charging_cost(energy_consumption, timestep, energy_prices):
        energy_cost = energy_prices[timestep]
        charging_cost = energy_cost * energy_consumption
        return charging_cost
    
def find_duplicate_ev(ev_station_assignment):
    # Create a reverse dictionary where keys are stations and values are lists of EVs
    station_ev_map = {}
    for ev, station in ev_station_assignment.items():
        if station is not None:
            station_ev_map.setdefault(station, []).append(ev)

    # Initialize a list to store EVs assigned to duplicate stations
    duplicate_evs = []

    # Iterate over the station_ev_map to find duplicate stations
    for station, ev_list in station_ev_map.items():
        if len(ev_list) > 1:
            duplicate_evs.extend(ev_list)

    return duplicate_evs

def env(**kwargs):
    env = raw_env(**kwargs)
    env = wrappers.AssertOutOfBoundsWrapper(env)
    env = wrappers.ClipOutOfBoundsWrapper(env)
    return env


class MultiAgentEVCharge(ParallelEnv):
    metadata = {
        "name": "Depot_env_v0"
    }
    
    def __init__(self, num_agents, render_mode = None):
        self.possible_agents = ["EV_" + str(r) for r in range(num_agents)]
        self.agent_name_mapping = dict(
            zip(self.possible_agents, list(range(len(self.possible_agents))))
        )

        # self.num_stations = random.randint(1, len(self.possible_agents))
        self.resolution = 15
        self.num_intervals = int((60/self.resolution)*24)
        self.battery_capacity = 450
        self.alpha = 10
        self.beta = 1
        self.gamma = 1
        self.delta = 0
        self.transformer_capacity = 1350 #1500kVa

    def reset(self, seed=None, options=None):
        #reset the environment to start again        
        flag = random.choice([1, 2, 3, 4])
        self.agents = self.possible_agents[:]
        self.rewards = {agent: 0 for agent in self.agents}
        self._cumulative_rewards = {agent: 0 for agent in self.agents}
        
        self.terminations = {agent: False for agent in self.agents}
        self.truncations = False
        self.infos = {agent: {} for agent in self.agents}
        self.state = {agent: None for agent in self.agents}
        self.observations = {agent: None for agent in self.agents}
        
        self.cummulative_charging_costs = 0
        self.completed_task_reward = 0
        
        
        self.timestep  = 0
        self.transformer_load = 0 
        self.num_stations = 4 #random.randint(1, len(self.agents))
        self.charging_station_availability = {station: True for station in range(self.num_stations)} 
    
        #Initialize EVs battery randomly between 10% and 60%
        self.SOC = {agent: random.uniform(45, 270) for agent in self.agents}
        #Determine each EVs next trip millage (Based on range of miles travelled by sandhaul)
        self.next_millage = {agent: random.randint(60, 205) for agent in self.agents} 
        #With the millage set the target SOC for next trip 
        self.target_SOC = {agent: get_target_SOC(self.next_millage[agent], self.battery_capacity) for agent in self.agents}
        #Randomize when EVs will depart for their first trip and time left at the depot
        self.departure_times = {agent: generate_departure_time(self.timestep + 1, self.num_intervals) for agent in self.agents}
        self.energy_prices = set_price_array(1, self.resolution, flag)
        self.assigned_station = {agent: None for agent in self.agents}
        self.complete_charge = 0
        self.action_spaces = {
            agent: Tuple((
                Box(low=0, high=350, shape=(1,), dtype=np.float32),
                Discrete(self.num_stations + 1)
            ))     
           for agent in self.agents 
        }
        
        self.observation_spaces = {
            agent: Dict(
                {
            "SOC": Box(low=0, high=350, shape=(1,), dtype=np.float32),
            "Target_SOC": Box(low=0, high=350, shape=(1,), dtype=np.float32),
            "Time_left_Depot": Discrete(self.num_intervals),
            # "Current_Energy_Price": Box(low=0, high=1, shape=(1,), dtype=np.float32),
            # "Future_Energy_Window": Box(low=0, high=1, shape=(len(self.energy_prices),), dtype=np.float32),
            # "Load_on_transformer": Box(low=0, high=np.inf, shape=(1,), dtype=np.float32),
            "Available_Stations": MultiBinary(self.num_stations)
        })
            for agent in self.possible_agents
            }
        

        self.departed_agents = []

        charging_stations_available = [1 if value else 0 for value in self.charging_station_availability.values()]
     
        for agent in self.agents:
            self.observations[agent] = {
            "SOC": np.array([self.SOC[agent]], dtype=np.float32),
            "Target_SOC": np.array([self.target_SOC[agent]], dtype=np.float32),
            "Time_left_Depot": np.array([self.departure_times[agent]], dtype=np.int32),
            # "Current_Energy_Price": np.array([self.energy_prices[self.timestep]], dtype=np.float32),
            # "Future_Energy_Window": np.array([self.energy_prices[self.timestep + i] for i in range(len(self.energy_prices) - self.timestep)], dtype=np.float32),
            # "Load_on_transformer": np.array([self.transformer_load], dtype=np.float32),
            "Available_Stations": np.array(charging_stations_available),
        }
        return self.observations, self.infos
    
    def step(self, actions):
        total_energy_consumed = 0
        charging_stations_available = [1 if value else 0 for value in self.charging_station_availability.values()]
        
        penalty_for_shared_station = 0
        reward_for_global_objective = 0
        reward_for_communication = 0 
        # self.terminations = {agent: False for agent in self.agents}
        infos = {agent: {} for agent in self.agents}
        potential_charge = {agent: 0 for agent in self.agents}
        potential_penalty = {agent: 0 for agent in self.agents}
        potential_rewards = {agent: 0 for agent in self.agents}
        total_episode_costs = 0 
        
        for agent in list(self.agents)[:]: # copy the agents list because we will remove some elements from it
            current_charging_cost = 0
            if self.timestep == self.departure_times[agent]:
                #print("agent leaving")
                if self.assigned_station[agent] != None:
                    station = self.assigned_station[agent]
                    self.charging_station_availability[station] = False
                    self.assigned_station[agent] = None
                    #remove agent from the environment
                    # remove_agent(self.agents, self.departed_agents, agent)
                self.terminations[agent] = True
                self.agents.remove(agent)
                self.departed_agents.append(agent)
                continue 

            
            #check if target SOC was met 
            if self.timestep == self.departure_times[agent] - 1:
                if self.SOC[agent] < self.target_SOC[agent]:
                    #if not assign_penalty
                    undercharge_penalty = (self.target_SOC[agent] - self.SOC[agent]) * self.alpha
                    for key in self.rewards:
                        self.rewards[key] -= undercharge_penalty 
                  
                    
                else:
                    #give reward for completing task
                    sufficient_battery_reward = 10
                   #calculate rewards and remove agent
                    for key in self.rewards:
                        self.rewards[key] += sufficient_battery_reward
                    self.completed_task_reward += sufficient_battery_reward
                    self.complete_charge += 1

            #Separate possible actions into charging rate and charge decision    

            # Extract number from string
            agent_idx = int(agent.split('_')[1])
            charge_rate, charge_decision = actions[agent_idx]
            charge_decision = T.argmax(charge_decision).item()
            charge_rate = charge_rate[0] * 350 #scaling back from 0 - 1 to 0 - 350
            if self.assigned_station[agent] != None: #Meaning agent is assigned to a station 
                if charge_decision == 0:
                #Remove agent from charging station and make available
                    station = self.assigned_station[agent]
                    self.charging_station_availability[station] = False
                    self.assigned_station[agent] = None
       
                else:
                    #charge the EVs battery 
                    energy_consumed = charge_EV(charge_rate, self.resolution)
                    self.SOC[agent] += energy_consumed
                    overcharge_penalty = 0
                    #Penalize if charging at a rate that would lead to overcharging the battery
                    if self.SOC[agent] > self.battery_capacity:
                        overcharge_penalty = (self.SOC[agent] - self.battery_capacity) * self.beta
                    
                    #Set battery to appropriate value     
                    self.SOC[agent] = min(self.SOC[agent], self.battery_capacity)
                    #calculate the charging cost
                    current_charging_cost = calculate_charging_cost(energy_consumed, self.timestep, self.energy_prices)

                    #get total costs for episode
                    total_episode_costs += current_charging_cost
                    total_energy_consumed +=  energy_consumed
                    
                    # self.rewards[agent] += (-current_charging_cost) - overcharge_penalty  
                    # self.rewards[agent] -= overcharge_penalty
                
            else:
                #If not assignedto a station, then the agent might be searching for a charger
                if charge_decision == 0:
                    #Do nothinng agent is idle for the timestep
                    continue
                    
                else:
                    #Determine charging station agent wants to access
                    station =  charge_decision - 1
                    overcharge_penalty = 0
                    #Determine if charging station is available
                    if self.charging_station_availability[station]:
                        #Assign agent to this station and add to occupied list
                        self.assigned_station[agent] = station
                        #potentially charge the vehicle
                        potential_charge[agent] = charge_EV(charge_rate, self.resolution)
                        self.SOC[agent] +=  potential_charge[agent]
                        
                        total_energy_consumed += potential_charge[agent]
                        
                        if self.SOC[agent] > self.battery_capacity:
                            extra = self.SOC[agent] - self.battery_capacity
                            overcharge_penalty = (self.SOC[agent] - self.battery_capacity) * self.beta
                            potential_charge[agent] = potential_charge[agent] - extra
                            #self.assigned_station[agent] = None

                        self.SOC[agent] = min(self.SOC[agent], self.battery_capacity)
                        # current_charging_cost = calculate_charging_cost(potential_charge[agent], self.timestep, self.energy_prices)
                        total_episode_costs += current_charging_cost
                        # potential_rewards[agent] += (-current_charging_cost) - overcharge_penalty
                    else: #penalize agent for selecting unavailable station
                        unavailable_station_penalty = self.gamma
                        self.rewards[agent] -= unavailable_station_penalty
                                                
        
        #Find agents that selected duplicate stations
        
        duplicates = find_duplicate_ev(self.assigned_station) #Returns a list of EVs with the same assigned station
        for duplicate in duplicates:
            # print(duplicate)
            #undo SOC charging
            self.SOC[duplicate] -= potential_charge[duplicate]
            total_energy_consumed -= potential_charge[duplicate]
            #undo rewards
            # self.rewards[duplicate] -= potential_rewards[duplicate]
            #add a penalty for overbooking:
            self.rewards[duplicate] -= self.gamma #this value will need to be adjusted to avoid agent of not doing anything
            current_charging_cost = calculate_charging_cost(potential_charge[duplicate], self.timestep, self.energy_prices)
            total_episode_costs =- current_charging_cost
            #Unassign them from the station 
            self.assigned_station[duplicate] = None
            
        #Make the stations that were occupied unavailable:
        for station in self.assigned_station.values():
            if station is not None:
                self.charging_station_availability[station] = False
    
        # #check for transformer overload and penalize only the charging agents
        # if total_energy_consumed > self.transformer_capacity:
        #     transformer_penalty =  ((total_energy_consumed - self.transformer_capacity)/self.transformer_capacity)*self.delta
        #     for agent in self.assigned_station:
        #         if self.assigned_station[agent] is not None:
        #             self.rewards[agent] -= transformer_penalty * self.delta
    
# Calculate the number of future timesteps available                
        self.timestep += 1
        remaining_timesteps = len(self.energy_prices) - self.timestep
        future_energy_window_array = np.full(len(self.energy_prices), fill_value=-1, dtype=np.float32)
        future_energy_window_array[self.timestep:] = self.energy_prices[:remaining_timesteps]
        self.transformer_load = total_energy_consumed
        charging_stations_available = [1 if value else 0 for value in self.charging_station_availability.values()]
        for agent in self.agents:
            time_left_depot = (self.departure_times[agent] - self.timestep)/self.num_intervals
        #set the new observations for the next timestep
        for agent in self.agents:
            
            time_left_depot = (self.departure_times[agent] - self.timestep)/self.num_intervals
            self.observations[agent] = {
                "SOC": np.array([self.SOC[agent]/self.battery_capacity], dtype=np.float32),
                "Target_SOC": np.array([self.target_SOC[agent]/self.battery_capacity], dtype=np.float32),
                "Time_left_Depot": np.array([time_left_depot], dtype=np.int32),
                # "Current_Energy_Price": np.array([self.energy_prices[self.timestep]], dtype=np.float32),
                # "Future_Energy_Window": np.array(future_energy_window_array),
                # "Load_on_transformer": np.array([self.transformer_load/self.transformer_capacity], dtype=np.float32),
                "Available_Stations": np.array(charging_stations_available)
            }
        #add any agents that are returning to depot back 
        # for agent in self.departed_agents:
        #     agent_reentry(self.departed_agents, agent, self.SOC, self.next_millage, self.target_SOC, self.agents, self.terminations)       
        self.cummulative_charging_costs += total_episode_costs
        #terminate the episode if the day is over
        if self.complete_charge == len(self.possible_agents):
            #Global reward for all  agents combined
            global_reward = 100
            for agent in self.possible_agents:
                self.rewards[agent] += global_reward
            
            
        if self.timestep == self.num_intervals - 1: 

            self.terminations = {agent: True for agent in self.possible_agents}
            self.agents.clear()
            self.truncations = True 

            # for agent in self.possible_agents:
            #     #Give a final reward of aggregated charging costs and positive feedback for every EV that reached its target SOC
            #     self.rewards[agent] +=  self.completed_task_reward
        # if not self.agents:
        #     self.truncations = True       
        return self.observations, self.rewards, self.terminations, self.truncations, self.infos
    
    def render(self):
        import tkinter as tk

        # Define the information for each EV
        ev_info = []
        
        for agent in self.agents:
            if self.assigned_station[agent] is not None:
                status = "Charging"
                assigned_station = self.assigned_station[agent] + 1
            else:
                status = "Idle"
                assigned_station  = None
                
            ev_dict = {"name": agent, "charge": round((self.SOC[agent] / self.battery_capacity) * 100, 1), "status": status, "station": assigned_station, "Target SOC": round((self.target_SOC[agent] / self.battery_capacity) * 100, 1), "Departure": self.departure_times[agent] }
            ev_info.append(ev_dict)
            
        # Define the available charging stations
        charging_stations = np.arange(1, self.num_stations + 1)

        # Function to get color code based on charge level
        def get_color(charge):
            if charge < 30:
                return "red"
            elif charge >= 30 and charge < 75:
                return "orange"
            else:
                return "green"

        # Create the main window
        root = tk.Tk()
        root.title("EV Information")

        # Define a function to create the grid layout
        def create_grid():
            # Loop over each EV and create a label with its information and meter visualization
            for i, ev in enumerate(ev_info):
                # Determine the row and column for the label
                row = i // 2
                col = i % 2
                
                # Create a frame to contain EV information and meter
                frame = tk.Frame(root, borderwidth=2, relief="solid")
                frame.grid(row=row, column=col, padx=5, pady=5, sticky="nsew")
                
                # Create a label with EV information
                if ev["status"] == "Charging":
                    label_text = f"{ev['name']}:\nCharge: {ev['charge']}%\nStatus: {ev['status']}\nStation: {ev['station']}\nTarget: {ev['Target SOC']}% \nDeparture: {ev['Departure']}"
                else:
                    label_text = f"{ev['name']}:\nCharge: {ev['charge']}%\nStatus: {ev['status']}\nTarget: {ev['Target SOC']} \nDeparture: {ev['Departure']}"
                label = tk.Label(frame, text=label_text, padx=10, pady=5, wraplength=150, anchor="center")
                label.grid(row=0, column=0, sticky="nsew")
                
                # Create a canvas for the meter visualization
                canvas = tk.Canvas(frame, width=100, height=20)
                canvas.grid(row=1, column=0, padx=10, pady=5, sticky="nsew")
                
                # Draw the meter background
                canvas.create_rectangle(0, 0, 100, 20, fill="lightgray", outline="")
                
                # Draw the meter fill based on charge level
                color = get_color(ev["charge"])
                canvas.create_rectangle(0, 0, ev["charge"], 20, fill=color, outline="")

        # Define a function to create the column for available charging stations
        def create_station_column():
            # Check if each charging station is occupied by an EV
            station_text = "Available Charging Stations:\n"
            for station in charging_stations:
                occupied = any(ev["station"] == station for ev in ev_info)
                availability = "Occupied" if occupied else "Available"
                station_text += f"Station {station}: {availability}\n"
            
            # Create a label to display the charging station information
            station_label = tk.Label(root, text=station_text, borderwidth=2, relief="solid", padx=10, pady=5, wraplength=150, anchor="center")
            station_label.grid(row=0, column=2, rowspan=len(ev_info) // 2 + 1, padx=5, pady=5, sticky="nsew")
        # Function to update the time step display
 
    # Function to update the time step display
        def update_time_step():
          # Convert timestep to hour of the day
            time_label.config(text=f"Current Timestep: {self.timestep}")
            
        
        root.after(500, update_time_step)  # Update every second
        # Call the functions to create the grid layout and the station column
        create_grid()
        create_station_column()
        
        time_label = tk.Label(root, text="", padx=5, pady=2, wraplength=100, anchor="center", font=("Arial", 10))
        time_label.grid(row=0, column=0, columnspan=2, sticky="ne")
        update_time_step()

        # Configure row and column weights to make them resize with the window
        for i in range(2):
            root.grid_columnconfigure(i, weight=1)
        for i in range((len(ev_info) + 1) // 2):
            root.grid_rowconfigure(i, weight=1)

        # Start the Tkinter event loop
        
        w = 800 # width for the Tk root
        h = 600 # height for the Tk root

        # get screen width and height
        ws = root.winfo_screenwidth() # width of the screen
        hs = root.winfo_screenheight() # height of the screen

        # calculate x and y coordinates for the Tk root window
        x = (ws/2) - (w/2)
        y = (hs/2) - (h/2)

        # set the dimensions of the screen 
        # and where it is placed
        root.geometry('%dx%d+%d+%d' % (w, h, x, y))

 
        root.after(1000, lambda: root.destroy()) # Destroy the widget after 30 seconds

        root.mainloop()
   
    
    def close():
        pass
    
    def observation_space(self, agent):
        return self.observation_spaces[agent]
    
    def action_space(self, agent):
        #return Tuple((Box(low=0, high=350, shape=(1,), dtype=np.float32),Discrete(num_stations + 1)))
        return self.action_spaces[agent]

# env = DepotEnv(num_agents=5)
# obs, infos = env.reset()

# Sample observations
# print("Sample Observations:")
# for agent in env.possible_agents:
#     obs = env.observations[agent]
#     print(f"{agent}:")
#     for key, value in obs.items():
#         print(f"\t{key}: {value}")

# Take one step in the environment
# observations, rewards, terminations, truncated, infos = env.step({agent: env.action_spaces[agent].sample() for agent in env.possible_agents})

# print("\nObservations after one step:")
# print(env.assigned_station)
# for agent in env.possible_agents:
#     obs = observations[agent]
#     print(f"{agent}:")
#     print(f"upcoming millage {env.next_millage[agent]}")
#     for key, value in obs.items():
#         print(f"\t{key}: {value}")
        
